{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yZvo8CHcetWN",
   "metadata": {
    "id": "yZvo8CHcetWN"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-shanghai",
   "metadata": {
    "id": "empty-shanghai"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wJmgAFK8_aCN",
   "metadata": {
    "id": "wJmgAFK8_aCN"
   },
   "source": [
    "Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n",
    "\n",
    "The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n",
    "\n",
    "OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3CWyKzKYAfEp",
   "metadata": {
    "id": "3CWyKzKYAfEp"
   },
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdq4cea5Akh8",
   "metadata": {
    "id": "cdq4cea5Akh8"
   },
   "source": [
    "In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n",
    "\n",
    "The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n",
    "\n",
    "* Facilitate the process of visa approvals.\n",
    "* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LpNqj6EzAhxy",
   "metadata": {
    "id": "LpNqj6EzAhxy"
   },
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uae5fMIyAnd-",
   "metadata": {
    "id": "uae5fMIyAnd-"
   },
   "source": [
    "The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n",
    "\n",
    "* case_id: ID of each visa application\n",
    "* continent: Information of continent the employee\n",
    "* education_of_employee: Information of education of the employee\n",
    "* has_job_experience: Does the employee has any job experience? Y= Yes; N = No\n",
    "* requires_job_training: Does the employee require any job training? Y = Yes; N = No\n",
    "* no_of_employees: Number of employees in the employer's company\n",
    "* yr_of_estab: Year in which the employer's company was established\n",
    "* region_of_employment: Information of foreign worker's intended region of employment in the US.\n",
    "* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\n",
    "* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n",
    "* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n",
    "* case_status:  Flag indicating if the Visa was certified or denied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lm7obbsV_RUT",
   "metadata": {
    "id": "Lm7obbsV_RUT"
   },
   "source": [
    "## Installing and Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6IOeGuQTMQXd",
   "metadata": {
    "id": "6IOeGuQTMQXd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "canadian-maple",
   "metadata": {
    "id": "canadian-maple"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrade build tools first\n",
    "%pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "# Py3.12-compatible pins\n",
    "%pip install -q \\\n",
    "  numpy==2.0.2 \\\n",
    "  pandas==2.2.2 \\\n",
    "  matplotlib==3.8.4 \\\n",
    "  seaborn==0.13.2 \\\n",
    "  scikit-learn==1.6.1 \\\n",
    "  sklearn-pandas==2.2.0 \\\n",
    "  xgboost==2.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OS2VAv465IZa",
   "metadata": {
    "id": "OS2VAv465IZa"
   },
   "source": [
    "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b64b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Installed package versions:\n",
      " * Python:     3.10.12\n",
      " * pandas:     2.2.2\n",
      " * numpy:      2.0.2\n",
      " * matplotlib: 3.8.4\n",
      " * seaborn:    0.13.2\n",
      " * scikit-learn: 1.6.1\n",
      " * xgboost:    2.0.3\n"
     ]
    }
   ],
   "source": [
    "# Core data science stack\n",
    "try:\n",
    "    import os\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np, pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except ImportError as e:\n",
    "    print(f\"[ERROR] Core package missing: {e.name}.\")\n",
    "    print(\"Solution: Install with pip, e.g.: !pip install pandas numpy matplotlib seaborn\")\n",
    "\n",
    "# Utilities\n",
    "try:\n",
    "    from pathlib import Path\n",
    "   # from google.colab import drive\n",
    "    from datetime import datetime\n",
    "    from collections import Counter\n",
    "except ImportError as e:\n",
    "    print(f\"[ERROR] Utility package missing: {e.name}.\")\n",
    "    print(\"Solution: If running outside Colab, remove or replace 'google.colab' imports.\")\n",
    "\n",
    "# Version verification\n",
    "try:\n",
    "    import pandas, numpy, matplotlib, seaborn, sklearn, xgboost\n",
    "    print(\"\\n[OK] Installed package versions:\")\n",
    "    print(f\" * Python:     {sys.version.split()[0]}\")\n",
    "    print(f\" * pandas:     {pandas.__version__}\")\n",
    "    print(f\" * numpy:      {numpy.__version__}\")\n",
    "    print(f\" * matplotlib: {matplotlib.__version__}\")\n",
    "    print(f\" * seaborn:    {seaborn.__version__}\")\n",
    "    print(f\" * scikit-learn: {sklearn.__version__}\")\n",
    "    print(f\" * xgboost:    {xgboost.__version__}\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Could not verify all package versions:\", e)\n",
    "    print(\"Double-check that packages are installed with pip/conda.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-passion",
   "metadata": {
    "id": "thorough-passion"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "q_MqHR8tN8mz",
   "metadata": {
    "id": "q_MqHR8tN8mz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Candidates:\n",
      "  - /mnt/c/Users/caste/OneDrive/Desktop/MLE/AI ML/EasyVisa.csv\n",
      "  - /mnt/c/Users/caste/Desktop/MLE/AI ML/EasyVisa.csv\n",
      "[OK] Using: /mnt/c/Users/caste/OneDrive/Desktop/MLE/AI ML/EasyVisa.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded EasyVisa.csv\n"
     ]
    }
   ],
   "source": [
    "# Primary WSL path Windows C: mounted at /mnt/c\n",
    "p1 = Path(\"/mnt/c/Users/caste/OneDrive/Desktop/MLE/AI ML/EasyVisa.csv\")\n",
    "# Fallback in case\n",
    "p2 = Path(\"/mnt/c/Users/caste/Desktop/MLE/AI ML/EasyVisa.csv\")\n",
    "\n",
    "data_path = p1 if p1.exists() else (p2 if p2.exists() else None)\n",
    "\n",
    "print(\"[INFO] Candidates:\")\n",
    "print(\"  -\", p1)\n",
    "print(\"  -\", p2)\n",
    "\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"[ERROR] Could not find EasyVisa.csv at either location above.\\n\"\n",
    "        \"Checks:\\n\"\n",
    "        \"  • Confirm the file is not cloud-only in OneDrive.\\n\"\n",
    "        \"  • Verify the path inside WSL\\n\"\n",
    "        \"  • If stored elsewhere, update the path here.\"\n",
    "    )\n",
    "\n",
    "print(f\"[OK] Using: {data_path}\")\n",
    "\n",
    "# Loader with encoding fallback\n",
    "def read_csv_safely(path: Path, **kwargs):\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"[WARN] UnicodeDecodeError for {path.name}. Retrying with latin-1 …\")\n",
    "        return pd.read_csv(path, encoding=\"latin-1\", engine=\"python\", **kwargs)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"[WARN] ParserError for {path.name}: {e}\\nTrying engine='python' with on_bad_lines='skip'.\")\n",
    "        return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", **kwargs)\n",
    "\n",
    "# Load\n",
    "df = read_csv_safely(data_path)\n",
    "print(f\"[OK] Loaded {data_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mq-1s9p-_aKl",
   "metadata": {
    "id": "mq-1s9p-_aKl"
   },
   "source": [
    "## Overview of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-wrist",
   "metadata": {
    "id": "aboriginal-wrist"
   },
   "source": [
    "#### View the first and last 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cVzRbi7oN6br",
   "metadata": {
    "id": "cVzRbi7oN6br"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b5ce th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_3b5ce_row0_col0, #T_3b5ce_row0_col1, #T_3b5ce_row0_col2, #T_3b5ce_row0_col3, #T_3b5ce_row0_col4, #T_3b5ce_row0_col5, #T_3b5ce_row0_col6, #T_3b5ce_row0_col7, #T_3b5ce_row0_col8, #T_3b5ce_row0_col9, #T_3b5ce_row0_col10, #T_3b5ce_row0_col11, #T_3b5ce_row1_col0, #T_3b5ce_row1_col1, #T_3b5ce_row1_col2, #T_3b5ce_row1_col3, #T_3b5ce_row1_col4, #T_3b5ce_row1_col5, #T_3b5ce_row1_col6, #T_3b5ce_row1_col7, #T_3b5ce_row1_col8, #T_3b5ce_row1_col9, #T_3b5ce_row1_col10, #T_3b5ce_row1_col11, #T_3b5ce_row2_col0, #T_3b5ce_row2_col1, #T_3b5ce_row2_col2, #T_3b5ce_row2_col3, #T_3b5ce_row2_col4, #T_3b5ce_row2_col5, #T_3b5ce_row2_col6, #T_3b5ce_row2_col7, #T_3b5ce_row2_col8, #T_3b5ce_row2_col9, #T_3b5ce_row2_col10, #T_3b5ce_row2_col11, #T_3b5ce_row3_col0, #T_3b5ce_row3_col1, #T_3b5ce_row3_col2, #T_3b5ce_row3_col3, #T_3b5ce_row3_col4, #T_3b5ce_row3_col5, #T_3b5ce_row3_col6, #T_3b5ce_row3_col7, #T_3b5ce_row3_col8, #T_3b5ce_row3_col9, #T_3b5ce_row3_col10, #T_3b5ce_row3_col11, #T_3b5ce_row4_col0, #T_3b5ce_row4_col1, #T_3b5ce_row4_col2, #T_3b5ce_row4_col3, #T_3b5ce_row4_col4, #T_3b5ce_row4_col5, #T_3b5ce_row4_col6, #T_3b5ce_row4_col7, #T_3b5ce_row4_col8, #T_3b5ce_row4_col9, #T_3b5ce_row4_col10, #T_3b5ce_row4_col11 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b5ce\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b5ce_level0_col0\" class=\"col_heading level0 col0\" >case_id</th>\n",
       "      <th id=\"T_3b5ce_level0_col1\" class=\"col_heading level0 col1\" >continent</th>\n",
       "      <th id=\"T_3b5ce_level0_col2\" class=\"col_heading level0 col2\" >education_of_employee</th>\n",
       "      <th id=\"T_3b5ce_level0_col3\" class=\"col_heading level0 col3\" >has_job_experience</th>\n",
       "      <th id=\"T_3b5ce_level0_col4\" class=\"col_heading level0 col4\" >requires_job_training</th>\n",
       "      <th id=\"T_3b5ce_level0_col5\" class=\"col_heading level0 col5\" >no_of_employees</th>\n",
       "      <th id=\"T_3b5ce_level0_col6\" class=\"col_heading level0 col6\" >yr_of_estab</th>\n",
       "      <th id=\"T_3b5ce_level0_col7\" class=\"col_heading level0 col7\" >region_of_employment</th>\n",
       "      <th id=\"T_3b5ce_level0_col8\" class=\"col_heading level0 col8\" >prevailing_wage</th>\n",
       "      <th id=\"T_3b5ce_level0_col9\" class=\"col_heading level0 col9\" >unit_of_wage</th>\n",
       "      <th id=\"T_3b5ce_level0_col10\" class=\"col_heading level0 col10\" >full_time_position</th>\n",
       "      <th id=\"T_3b5ce_level0_col11\" class=\"col_heading level0 col11\" >case_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3b5ce_row0_col0\" class=\"data row0 col0\" >EZYV21644</td>\n",
       "      <td id=\"T_3b5ce_row0_col1\" class=\"data row0 col1\" >Europe</td>\n",
       "      <td id=\"T_3b5ce_row0_col2\" class=\"data row0 col2\" >Bachelor's</td>\n",
       "      <td id=\"T_3b5ce_row0_col3\" class=\"data row0 col3\" >N</td>\n",
       "      <td id=\"T_3b5ce_row0_col4\" class=\"data row0 col4\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row0_col5\" class=\"data row0 col5\" >3126</td>\n",
       "      <td id=\"T_3b5ce_row0_col6\" class=\"data row0 col6\" >1800</td>\n",
       "      <td id=\"T_3b5ce_row0_col7\" class=\"data row0 col7\" >South</td>\n",
       "      <td id=\"T_3b5ce_row0_col8\" class=\"data row0 col8\" >192806.060000</td>\n",
       "      <td id=\"T_3b5ce_row0_col9\" class=\"data row0 col9\" >Year</td>\n",
       "      <td id=\"T_3b5ce_row0_col10\" class=\"data row0 col10\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row0_col11\" class=\"data row0 col11\" >Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3b5ce_row1_col0\" class=\"data row1 col0\" >EZYV24364</td>\n",
       "      <td id=\"T_3b5ce_row1_col1\" class=\"data row1 col1\" >Asia</td>\n",
       "      <td id=\"T_3b5ce_row1_col2\" class=\"data row1 col2\" >Master's</td>\n",
       "      <td id=\"T_3b5ce_row1_col3\" class=\"data row1 col3\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row1_col4\" class=\"data row1 col4\" >N</td>\n",
       "      <td id=\"T_3b5ce_row1_col5\" class=\"data row1 col5\" >1649</td>\n",
       "      <td id=\"T_3b5ce_row1_col6\" class=\"data row1 col6\" >1800</td>\n",
       "      <td id=\"T_3b5ce_row1_col7\" class=\"data row1 col7\" >Midwest</td>\n",
       "      <td id=\"T_3b5ce_row1_col8\" class=\"data row1 col8\" >148321.570000</td>\n",
       "      <td id=\"T_3b5ce_row1_col9\" class=\"data row1 col9\" >Year</td>\n",
       "      <td id=\"T_3b5ce_row1_col10\" class=\"data row1 col10\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row1_col11\" class=\"data row1 col11\" >Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3b5ce_row2_col0\" class=\"data row2 col0\" >EZYV22098</td>\n",
       "      <td id=\"T_3b5ce_row2_col1\" class=\"data row2 col1\" >Asia</td>\n",
       "      <td id=\"T_3b5ce_row2_col2\" class=\"data row2 col2\" >Bachelor's</td>\n",
       "      <td id=\"T_3b5ce_row2_col3\" class=\"data row2 col3\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row2_col4\" class=\"data row2 col4\" >N</td>\n",
       "      <td id=\"T_3b5ce_row2_col5\" class=\"data row2 col5\" >2043</td>\n",
       "      <td id=\"T_3b5ce_row2_col6\" class=\"data row2 col6\" >1800</td>\n",
       "      <td id=\"T_3b5ce_row2_col7\" class=\"data row2 col7\" >Northeast</td>\n",
       "      <td id=\"T_3b5ce_row2_col8\" class=\"data row2 col8\" >145985.430000</td>\n",
       "      <td id=\"T_3b5ce_row2_col9\" class=\"data row2 col9\" >Year</td>\n",
       "      <td id=\"T_3b5ce_row2_col10\" class=\"data row2 col10\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row2_col11\" class=\"data row2 col11\" >Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3b5ce_row3_col0\" class=\"data row3 col0\" >EZYV12103</td>\n",
       "      <td id=\"T_3b5ce_row3_col1\" class=\"data row3 col1\" >Asia</td>\n",
       "      <td id=\"T_3b5ce_row3_col2\" class=\"data row3 col2\" >High School</td>\n",
       "      <td id=\"T_3b5ce_row3_col3\" class=\"data row3 col3\" >N</td>\n",
       "      <td id=\"T_3b5ce_row3_col4\" class=\"data row3 col4\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row3_col5\" class=\"data row3 col5\" >808</td>\n",
       "      <td id=\"T_3b5ce_row3_col6\" class=\"data row3 col6\" >1800</td>\n",
       "      <td id=\"T_3b5ce_row3_col7\" class=\"data row3 col7\" >West</td>\n",
       "      <td id=\"T_3b5ce_row3_col8\" class=\"data row3 col8\" >127303.960000</td>\n",
       "      <td id=\"T_3b5ce_row3_col9\" class=\"data row3 col9\" >Year</td>\n",
       "      <td id=\"T_3b5ce_row3_col10\" class=\"data row3 col10\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row3_col11\" class=\"data row3 col11\" >Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b5ce_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3b5ce_row4_col0\" class=\"data row4 col0\" >EZYV13156</td>\n",
       "      <td id=\"T_3b5ce_row4_col1\" class=\"data row4 col1\" >Asia</td>\n",
       "      <td id=\"T_3b5ce_row4_col2\" class=\"data row4 col2\" >Master's</td>\n",
       "      <td id=\"T_3b5ce_row4_col3\" class=\"data row4 col3\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row4_col4\" class=\"data row4 col4\" >N</td>\n",
       "      <td id=\"T_3b5ce_row4_col5\" class=\"data row4 col5\" >573</td>\n",
       "      <td id=\"T_3b5ce_row4_col6\" class=\"data row4 col6\" >1800</td>\n",
       "      <td id=\"T_3b5ce_row4_col7\" class=\"data row4 col7\" >Northeast</td>\n",
       "      <td id=\"T_3b5ce_row4_col8\" class=\"data row4 col8\" >124457.500000</td>\n",
       "      <td id=\"T_3b5ce_row4_col9\" class=\"data row4 col9\" >Year</td>\n",
       "      <td id=\"T_3b5ce_row4_col10\" class=\"data row4 col10\" >Y</td>\n",
       "      <td id=\"T_3b5ce_row4_col11\" class=\"data row4 col11\" >Certified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75ef6f0edff0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset first 5 rows\n",
    "display(df.head()\n",
    "        .style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c13778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f53e7 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f53e7_row0_col0, #T_f53e7_row0_col1, #T_f53e7_row0_col2, #T_f53e7_row0_col3, #T_f53e7_row0_col4, #T_f53e7_row0_col5, #T_f53e7_row0_col6, #T_f53e7_row0_col7, #T_f53e7_row0_col8, #T_f53e7_row0_col9, #T_f53e7_row0_col10, #T_f53e7_row0_col11, #T_f53e7_row1_col0, #T_f53e7_row1_col1, #T_f53e7_row1_col2, #T_f53e7_row1_col3, #T_f53e7_row1_col4, #T_f53e7_row1_col5, #T_f53e7_row1_col6, #T_f53e7_row1_col7, #T_f53e7_row1_col8, #T_f53e7_row1_col9, #T_f53e7_row1_col10, #T_f53e7_row1_col11, #T_f53e7_row2_col0, #T_f53e7_row2_col1, #T_f53e7_row2_col2, #T_f53e7_row2_col3, #T_f53e7_row2_col4, #T_f53e7_row2_col5, #T_f53e7_row2_col6, #T_f53e7_row2_col7, #T_f53e7_row2_col8, #T_f53e7_row2_col9, #T_f53e7_row2_col10, #T_f53e7_row2_col11, #T_f53e7_row3_col0, #T_f53e7_row3_col1, #T_f53e7_row3_col2, #T_f53e7_row3_col3, #T_f53e7_row3_col4, #T_f53e7_row3_col5, #T_f53e7_row3_col6, #T_f53e7_row3_col7, #T_f53e7_row3_col8, #T_f53e7_row3_col9, #T_f53e7_row3_col10, #T_f53e7_row3_col11, #T_f53e7_row4_col0, #T_f53e7_row4_col1, #T_f53e7_row4_col2, #T_f53e7_row4_col3, #T_f53e7_row4_col4, #T_f53e7_row4_col5, #T_f53e7_row4_col6, #T_f53e7_row4_col7, #T_f53e7_row4_col8, #T_f53e7_row4_col9, #T_f53e7_row4_col10, #T_f53e7_row4_col11 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f53e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f53e7_level0_col0\" class=\"col_heading level0 col0\" >case_id</th>\n",
       "      <th id=\"T_f53e7_level0_col1\" class=\"col_heading level0 col1\" >continent</th>\n",
       "      <th id=\"T_f53e7_level0_col2\" class=\"col_heading level0 col2\" >education_of_employee</th>\n",
       "      <th id=\"T_f53e7_level0_col3\" class=\"col_heading level0 col3\" >has_job_experience</th>\n",
       "      <th id=\"T_f53e7_level0_col4\" class=\"col_heading level0 col4\" >requires_job_training</th>\n",
       "      <th id=\"T_f53e7_level0_col5\" class=\"col_heading level0 col5\" >no_of_employees</th>\n",
       "      <th id=\"T_f53e7_level0_col6\" class=\"col_heading level0 col6\" >yr_of_estab</th>\n",
       "      <th id=\"T_f53e7_level0_col7\" class=\"col_heading level0 col7\" >region_of_employment</th>\n",
       "      <th id=\"T_f53e7_level0_col8\" class=\"col_heading level0 col8\" >prevailing_wage</th>\n",
       "      <th id=\"T_f53e7_level0_col9\" class=\"col_heading level0 col9\" >unit_of_wage</th>\n",
       "      <th id=\"T_f53e7_level0_col10\" class=\"col_heading level0 col10\" >full_time_position</th>\n",
       "      <th id=\"T_f53e7_level0_col11\" class=\"col_heading level0 col11\" >case_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f53e7_level0_row0\" class=\"row_heading level0 row0\" >25475</th>\n",
       "      <td id=\"T_f53e7_row0_col0\" class=\"data row0 col0\" >EZYV5163</td>\n",
       "      <td id=\"T_f53e7_row0_col1\" class=\"data row0 col1\" >Asia</td>\n",
       "      <td id=\"T_f53e7_row0_col2\" class=\"data row0 col2\" >Bachelor's</td>\n",
       "      <td id=\"T_f53e7_row0_col3\" class=\"data row0 col3\" >Y</td>\n",
       "      <td id=\"T_f53e7_row0_col4\" class=\"data row0 col4\" >N</td>\n",
       "      <td id=\"T_f53e7_row0_col5\" class=\"data row0 col5\" >500</td>\n",
       "      <td id=\"T_f53e7_row0_col6\" class=\"data row0 col6\" >2016</td>\n",
       "      <td id=\"T_f53e7_row0_col7\" class=\"data row0 col7\" >Northeast</td>\n",
       "      <td id=\"T_f53e7_row0_col8\" class=\"data row0 col8\" >12476.400000</td>\n",
       "      <td id=\"T_f53e7_row0_col9\" class=\"data row0 col9\" >Year</td>\n",
       "      <td id=\"T_f53e7_row0_col10\" class=\"data row0 col10\" >Y</td>\n",
       "      <td id=\"T_f53e7_row0_col11\" class=\"data row0 col11\" >Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f53e7_level0_row1\" class=\"row_heading level0 row1\" >25476</th>\n",
       "      <td id=\"T_f53e7_row1_col0\" class=\"data row1 col0\" >EZYV15379</td>\n",
       "      <td id=\"T_f53e7_row1_col1\" class=\"data row1 col1\" >Europe</td>\n",
       "      <td id=\"T_f53e7_row1_col2\" class=\"data row1 col2\" >Doctorate</td>\n",
       "      <td id=\"T_f53e7_row1_col3\" class=\"data row1 col3\" >N</td>\n",
       "      <td id=\"T_f53e7_row1_col4\" class=\"data row1 col4\" >N</td>\n",
       "      <td id=\"T_f53e7_row1_col5\" class=\"data row1 col5\" >41</td>\n",
       "      <td id=\"T_f53e7_row1_col6\" class=\"data row1 col6\" >2016</td>\n",
       "      <td id=\"T_f53e7_row1_col7\" class=\"data row1 col7\" >Northeast</td>\n",
       "      <td id=\"T_f53e7_row1_col8\" class=\"data row1 col8\" >9442.230000</td>\n",
       "      <td id=\"T_f53e7_row1_col9\" class=\"data row1 col9\" >Year</td>\n",
       "      <td id=\"T_f53e7_row1_col10\" class=\"data row1 col10\" >Y</td>\n",
       "      <td id=\"T_f53e7_row1_col11\" class=\"data row1 col11\" >Certified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f53e7_level0_row2\" class=\"row_heading level0 row2\" >25477</th>\n",
       "      <td id=\"T_f53e7_row2_col0\" class=\"data row2 col0\" >EZYV15437</td>\n",
       "      <td id=\"T_f53e7_row2_col1\" class=\"data row2 col1\" >Europe</td>\n",
       "      <td id=\"T_f53e7_row2_col2\" class=\"data row2 col2\" >Bachelor's</td>\n",
       "      <td id=\"T_f53e7_row2_col3\" class=\"data row2 col3\" >Y</td>\n",
       "      <td id=\"T_f53e7_row2_col4\" class=\"data row2 col4\" >N</td>\n",
       "      <td id=\"T_f53e7_row2_col5\" class=\"data row2 col5\" >1338</td>\n",
       "      <td id=\"T_f53e7_row2_col6\" class=\"data row2 col6\" >2016</td>\n",
       "      <td id=\"T_f53e7_row2_col7\" class=\"data row2 col7\" >West</td>\n",
       "      <td id=\"T_f53e7_row2_col8\" class=\"data row2 col8\" >639.957100</td>\n",
       "      <td id=\"T_f53e7_row2_col9\" class=\"data row2 col9\" >Hour</td>\n",
       "      <td id=\"T_f53e7_row2_col10\" class=\"data row2 col10\" >Y</td>\n",
       "      <td id=\"T_f53e7_row2_col11\" class=\"data row2 col11\" >Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f53e7_level0_row3\" class=\"row_heading level0 row3\" >25478</th>\n",
       "      <td id=\"T_f53e7_row3_col0\" class=\"data row3 col0\" >EZYV20413</td>\n",
       "      <td id=\"T_f53e7_row3_col1\" class=\"data row3 col1\" >Asia</td>\n",
       "      <td id=\"T_f53e7_row3_col2\" class=\"data row3 col2\" >High School</td>\n",
       "      <td id=\"T_f53e7_row3_col3\" class=\"data row3 col3\" >N</td>\n",
       "      <td id=\"T_f53e7_row3_col4\" class=\"data row3 col4\" >N</td>\n",
       "      <td id=\"T_f53e7_row3_col5\" class=\"data row3 col5\" >1655</td>\n",
       "      <td id=\"T_f53e7_row3_col6\" class=\"data row3 col6\" >2016</td>\n",
       "      <td id=\"T_f53e7_row3_col7\" class=\"data row3 col7\" >Northeast</td>\n",
       "      <td id=\"T_f53e7_row3_col8\" class=\"data row3 col8\" >538.996600</td>\n",
       "      <td id=\"T_f53e7_row3_col9\" class=\"data row3 col9\" >Hour</td>\n",
       "      <td id=\"T_f53e7_row3_col10\" class=\"data row3 col10\" >Y</td>\n",
       "      <td id=\"T_f53e7_row3_col11\" class=\"data row3 col11\" >Denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f53e7_level0_row4\" class=\"row_heading level0 row4\" >25479</th>\n",
       "      <td id=\"T_f53e7_row4_col0\" class=\"data row4 col0\" >EZYV8640</td>\n",
       "      <td id=\"T_f53e7_row4_col1\" class=\"data row4 col1\" >Europe</td>\n",
       "      <td id=\"T_f53e7_row4_col2\" class=\"data row4 col2\" >Master's</td>\n",
       "      <td id=\"T_f53e7_row4_col3\" class=\"data row4 col3\" >N</td>\n",
       "      <td id=\"T_f53e7_row4_col4\" class=\"data row4 col4\" >N</td>\n",
       "      <td id=\"T_f53e7_row4_col5\" class=\"data row4 col5\" >2089</td>\n",
       "      <td id=\"T_f53e7_row4_col6\" class=\"data row4 col6\" >2016</td>\n",
       "      <td id=\"T_f53e7_row4_col7\" class=\"data row4 col7\" >West</td>\n",
       "      <td id=\"T_f53e7_row4_col8\" class=\"data row4 col8\" >399.629700</td>\n",
       "      <td id=\"T_f53e7_row4_col9\" class=\"data row4 col9\" >Hour</td>\n",
       "      <td id=\"T_f53e7_row4_col10\" class=\"data row4 col10\" >Y</td>\n",
       "      <td id=\"T_f53e7_row4_col11\" class=\"data row4 col11\" >Denied</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75efeeb66800>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset last 5 rows\n",
    "display(df.tail()\n",
    "        .style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-camel",
   "metadata": {
    "id": "accessory-camel"
   },
   "source": [
    "#### Understand the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "Ym8ApC21N64n",
   "metadata": {
    "id": "Ym8ApC21N64n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Dataset shape: (25480, 12)\n"
     ]
    }
   ],
   "source": [
    "# Print the rows and coulmns\n",
    "print(f\"\\n[INFO] Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-berkeley",
   "metadata": {
    "id": "assigned-berkeley"
   },
   "source": [
    "#### Check the data types of the columns for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ekk0QEpXN7im",
   "metadata": {
    "id": "ekk0QEpXN7im"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25480 entries, 0 to 25479\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   case_id                25480 non-null  object \n",
      " 1   continent              25480 non-null  object \n",
      " 2   education_of_employee  25480 non-null  object \n",
      " 3   has_job_experience     25480 non-null  object \n",
      " 4   requires_job_training  25480 non-null  object \n",
      " 5   no_of_employees        25480 non-null  int64  \n",
      " 6   yr_of_estab            25480 non-null  int64  \n",
      " 7   region_of_employment   25480 non-null  object \n",
      " 8   prevailing_wage        25480 non-null  float64\n",
      " 9   unit_of_wage           25480 non-null  object \n",
      " 10  full_time_position     25480 non-null  object \n",
      " 11  case_status            25480 non-null  object \n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-horizontal",
   "metadata": {
    "id": "standing-horizontal"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9020186",
   "metadata": {},
   "source": [
    "Learned to set the data up before hand to avoid pitfalls later down the pipline\n",
    "\n",
    "* Rock-solid config & verification (RNG, target integrity, reproducibility)\n",
    "\n",
    "* Canonical maps for messy strings (target, Y/N flags, categories)\n",
    "\n",
    "* A schema guard that prints unknown headers, missing canon columns, and collisions\n",
    "\n",
    "* A single build_canonical_df(df) entrypoint returning a clean, model-ready frame\n",
    "\n",
    "* Stratified split sanity and class-balance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3bb09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VERIFICATION START: EasyVisa Raw ===\n",
      "[OK] RNG reproducibility\n",
      "[OK] Target column present\n",
      "[OK] All expected columns present\n",
      "[SUMMARY] rows=25480 cols=12 | expected_ok=True  ⇒  PASS ✓\n",
      "=== VERIFICATION END: EasyVisa Raw ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A) CONFIG & VERIFICATION (Pro)\n",
    "import re, difflib\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Config ---\n",
    "RNG = 72\n",
    "np.random.seed(RNG)\n",
    "\n",
    "TARGET_RAW = \"case_status\"     # raw target column in EasyVisa\n",
    "TARGET_LBL = \"_case_label\"     # canonical string label: 'Certified' / 'Denied'\n",
    "TARGET_BIN = \"y\"               # numeric target: 1/0\n",
    "\n",
    "# Minimal raw columns expected before any feature engineering\n",
    "EXPECTED_COLS = {\n",
    "    \"case_id\",\"continent\",\"education_of_employee\",\"has_job_experience\",\n",
    "    \"requires_job_training\",\"no_of_employees\",\"yr_of_estab\",\n",
    "    \"region_of_employment\",\"prevailing_wage\",\"unit_of_wage\",\n",
    "    \"full_time_position\",\"case_status\"\n",
    "}\n",
    "\n",
    "# --- Utilities ---\n",
    "def verify_rng(seed: int = RNG) -> None:\n",
    "    \"\"\"Deterministically verify RNG reproducibility.\"\"\"\n",
    "    a = np.random.RandomState(seed).rand(5)\n",
    "    b = np.random.RandomState(seed).rand(5)\n",
    "    assert np.allclose(a, b), \"RNG reproducibility failed\"\n",
    "    print(\"[OK] RNG reproducibility\")\n",
    "\n",
    "def verify_target_exists(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Ensure the raw target column is present.\"\"\"\n",
    "    assert TARGET_RAW in df.columns, f\"Missing target column `{TARGET_RAW}`\"\n",
    "    print(\"[OK] Target column present\")\n",
    "\n",
    "def report_class_balance(y: pd.Series, title: str = \"[INFO] class balance\") -> Dict[str, Any]:\n",
    "    \"\"\"Print and return class balance + imbalance ratio (if binary).\"\"\"\n",
    "    vc = y.value_counts(dropna=False)\n",
    "    ratio = float(vc.max()/vc.min()) if (len(vc) == 2 and vc.min() > 0) else float(\"nan\")\n",
    "    print(f\"{title} →\")\n",
    "    print(vc.to_string())\n",
    "    if np.isfinite(ratio):\n",
    "        print(f\"  Imbalance ratio ≈ {ratio:.2f}:1\")\n",
    "    return {\"counts\": vc.to_dict(), \"ratio\": ratio}\n",
    "\n",
    "def verify_expected_columns(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Check presence of all expected raw columns; print concise status.\"\"\"\n",
    "    missing = sorted([c for c in EXPECTED_COLS if c not in df.columns])\n",
    "    extra   = sorted([c for c in df.columns if c not in EXPECTED_COLS])\n",
    "    if missing:\n",
    "        print(f\"[WARN] Missing expected columns: {missing}\")\n",
    "    else:\n",
    "        print(\"[OK] All expected columns present\")\n",
    "    if extra:\n",
    "        print(f\"[INFO] Extra columns (not required): {extra[:10]}{' …' if len(extra) > 10 else ''}\")\n",
    "    return {\"missing\": missing, \"extra\": extra, \"passed\": len(missing) == 0}\n",
    "\n",
    "def verify_no_nans(series: pd.Series, name: str) -> None:\n",
    "    \"\"\"Hard guard to ensure no NaNs in a required series.\"\"\"\n",
    "    n = int(series.isna().sum())\n",
    "    print(f\"[VERIFY] {name}: NaN count = {n}\")\n",
    "    assert n == 0, f\"Found NaNs in `{name}`\"\n",
    "\n",
    "# --- One-call dataset verifier ---\n",
    "def verify_dataset(df: pd.DataFrame, name: str = \"EasyVisa\", strict: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    End-to-end verification for the raw EasyVisa frame.\n",
    "    - RNG reproducibility\n",
    "    - Target column exists\n",
    "    - Expected raw columns present\n",
    "    - (Optional) class balance if a canonical/encoded target is available\n",
    "    Returns a dict report; raises if strict=True and checks fail.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== VERIFICATION START: {name} ===\")\n",
    "    verify_rng()\n",
    "    verify_target_exists(df)\n",
    "    schema_rep = verify_expected_columns(df)\n",
    "\n",
    "    # If the notebook has already created canonical target columns, report them\n",
    "    report_lbl = {}\n",
    "    report_bin = {}\n",
    "    if TARGET_LBL in df.columns:\n",
    "        report_lbl = report_class_balance(df[TARGET_LBL], \"[INFO] target label balance\")\n",
    "    if TARGET_BIN in df.columns:\n",
    "        report_bin = report_class_balance(df[TARGET_BIN], \"[INFO] target numeric balance\")\n",
    "\n",
    "    passed = bool(schema_rep[\"passed\"])\n",
    "    summary = {\n",
    "        \"dataset\": name,\n",
    "        \"rows\": int(len(df)),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"expected_ok\": passed,\n",
    "        \"missing\": schema_rep[\"missing\"],\n",
    "        \"extra\": schema_rep[\"extra\"],\n",
    "        \"label_balance\": report_lbl,\n",
    "        \"numeric_balance\": report_bin,\n",
    "        \"passed\": passed,\n",
    "    }\n",
    "\n",
    "    status = \"PASS ✓\" if summary[\"passed\"] else \"FAIL ✗\"\n",
    "    print(f\"[SUMMARY] rows={summary['rows']} cols={summary['cols']} | \"\n",
    "          f\"expected_ok={summary['expected_ok']}  ⇒  {status}\")\n",
    "    print(f\"=== VERIFICATION END: {name} ===\\n\")\n",
    "\n",
    "    if strict and not summary[\"passed\"]:\n",
    "        raise AssertionError(f\"[{name}] Verification failed: {summary}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "report = verify_dataset(df, name=\"EasyVisa Raw\", strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4f25f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EasyVisa-cols] No unknown headers. ✓\n",
      "[EasyVisa-cols] MISSING expected canonical columns: None ✓\n",
      "[EasyVisa-cols] No collisions. ✓\n",
      "[EasyVisa-cols] COLUMN SUMMARY → unknown=0 | missing=0 | collisions=0 ⇒ PASS ✓\n",
      "[EasyVisa-vals] TARGET label counts: {'Certified': 17018, 'Denied': 8462}\n",
      "[EasyVisa-vals] TARGET numeric counts: {np.int64(1): 17018, np.int64(0): 8462}\n",
      "[EasyVisa-vals] TARGET NaNs: 0\n",
      "[EasyVisa-vals] YN invalid → job_exp=0 | job_train=0 | full_time=0\n",
      "[EasyVisa-vals] Education 'Other' share: 0.0%\n",
      "[EasyVisa-vals] VALUE SUMMARY ⇒ PASS ✓\n",
      "[EasyVisa] CANONICALIZATION SUMMARY → columns=True & values=True ⇒ PASS ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B) CANONICALIZATION \n",
    "from typing import Dict, Any, Tuple\n",
    "import difflib, re\n",
    "\n",
    "# Helpers\n",
    "_norm = lambda s: re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "\n",
    "_CANON_MAP: Dict[str,str] = {\n",
    "    \"caseid\": \"case_id\",\n",
    "    \"continent\": \"continent\",\n",
    "    \"educationofemployee\": \"education_of_employee\",\n",
    "    \"hasjobexperience\": \"has_job_experience\",\n",
    "    \"requiresjobtraining\": \"requires_job_training\",\n",
    "    \"noofemployees\": \"no_of_employees\",\n",
    "    \"yrofestab\": \"yr_of_estab\",\n",
    "    \"regionofemployment\": \"region_of_employment\",\n",
    "    \"prevailingwage\": \"prevailing_wage\",\n",
    "    \"unitofwage\": \"unit_of_wage\",\n",
    "    \"fulltimeposition\": \"full_time_position\",\n",
    "    \"casestatus\": \"case_status\",\n",
    "}\n",
    "\n",
    "def canon_case_status(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\").str.strip().str.lower()\n",
    "    m = {\n",
    "        \"certified\": \"Certified\",\n",
    "        \"certified-expired\": \"Certified\",\n",
    "        \"denied\": \"Denied\",\n",
    "        \"rejected\": \"Denied\",\n",
    "        \"withdrawn\": \"Denied\",\n",
    "    }\n",
    "    return s.map(m)\n",
    "\n",
    "def canon_yn(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\").str.strip().str.upper().replace({\"YES\":\"Y\",\"NO\":\"N\"})\n",
    "    return s.where(s.isin([\"Y\",\"N\"]), np.nan)\n",
    "\n",
    "def canon_title(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(\"string\").str.strip().str.title()\n",
    "\n",
    "def canon_education(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\").str.strip().str.lower()\n",
    "    s = s.str.replace(\"’\",\"'\", regex=False).str.replace(r\"\\s+\",\" \", regex=True)\n",
    "    def map_one(x):\n",
    "        if x is None or x == \"nan\": return np.nan\n",
    "        if \"high\"      in x: return \"High School\"\n",
    "        if \"bachelor\"  in x: return \"Bachelor's\"\n",
    "        if \"master\"    in x: return \"Master's\"\n",
    "        if \"doctor\" in x or \"phd\" in x: return \"Doctorate\"\n",
    "        return \"Other\"\n",
    "    return s.map(map_one)\n",
    "\n",
    "# Internal columns ignore schema warnings \n",
    "INTERNAL_COLS = {\n",
    "    \"_case_label\",\"_edu\",\"_continent\",\"_region\",\"_uow\",\n",
    "    \"_job_exp\",\"_job_train\",\"_full_time\"\n",
    "}\n",
    "\n",
    "# Column-level canonicalization \n",
    "def canonicalize_columns(df: pd.DataFrame, name: str = \"frame\") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Rename raw columns using _CANON_MAP, report unknowns/missing/collisions.\"\"\"\n",
    "    raw = list(df.columns)\n",
    "    keys = [_norm(c) for c in raw]\n",
    "\n",
    "    # rename with collision protection\n",
    "    ren, used = {}, set()\n",
    "    for raw_col, k in zip(raw, keys):\n",
    "        new = _CANON_MAP.get(k, raw_col)\n",
    "        if new in used and new != raw_col:\n",
    "            new = raw_col  # avoid overwrite\n",
    "        ren[raw_col] = new\n",
    "        used.add(new)\n",
    "    out = df.rename(columns=ren).copy()\n",
    "\n",
    "    # unknown headers skip internals expected names are the canonical keys’ values\n",
    "    expected = set(_CANON_MAP.values())\n",
    "    unknown = [(r, k) for r, k in zip(raw, keys)\n",
    "               if r not in INTERNAL_COLS and not str(r).startswith(\"_\")\n",
    "               and (k not in _CANON_MAP) and (r not in expected)]\n",
    "\n",
    "    # missing expected canonical columns\n",
    "    missing = sorted([c for c in expected if c not in out.columns])\n",
    "\n",
    "    # collisions two raw normalize to same key\n",
    "    seen, collisions = {}, {}\n",
    "    for r, k in zip(raw, keys):\n",
    "        if k in seen and seen[k] != r:\n",
    "            collisions.setdefault(k, set()).update([seen[k], r])\n",
    "        else:\n",
    "            seen[k] = r\n",
    "\n",
    "    # print audit\n",
    "    if unknown:\n",
    "        print(f\"[{name}] UNKNOWN headers:\", [r for r,_ in unknown])\n",
    "        print(f\"[{name}] Suggested _CANON_MAP patches:\")\n",
    "        for r,k in unknown:\n",
    "            guess = difflib.get_close_matches(k, list(_CANON_MAP.keys()), n=1, cutoff=0.6)\n",
    "            guess_str = guess[0] if guess else \"<add-key>\"\n",
    "            print(f'    _CANON_MAP[\"{k}\"] = \"{r}\"   # raw=\"{r}\"')\n",
    "    else:\n",
    "        print(f\"[{name}] No unknown headers. ✓\")\n",
    "\n",
    "    print(f\"[{name}] MISSING expected canonical columns:\", missing if missing else \"None ✓\")\n",
    "\n",
    "    if collisions:\n",
    "        print(f\"[{name}] COLLISIONS:\")\n",
    "        for k, raws in collisions.items():\n",
    "            print(f\"    norm='{k}' ← raw {sorted(list(raws))}\")\n",
    "    else:\n",
    "        print(f\"[{name}] No collisions. ✓\")\n",
    "\n",
    "    report = {\n",
    "        \"dataset\": name,\n",
    "        \"unknown\": [r for r,_ in unknown],\n",
    "        \"unknown_count\": len(unknown),\n",
    "        \"missing\": missing,\n",
    "        \"missing_count\": len(missing),\n",
    "        \"collisions\": {k: sorted(list(v)) for k,v in collisions.items()},\n",
    "        \"collision_count\": len(collisions),\n",
    "        \"passed\": (len(unknown) == 0 and len(missing) == 0 and len(collisions) == 0)\n",
    "    }\n",
    "    status = \"PASS ✓\" if report[\"passed\"] else \"WARN ✗\"\n",
    "    print(f\"[{name}] COLUMN SUMMARY → unknown={report['unknown_count']} | \"\n",
    "          f\"missing={report['missing_count']} | collisions={report['collision_count']} ⇒ {status}\")\n",
    "    return out, report\n",
    "\n",
    "# Value-level canonicalization \n",
    "def apply_canonical_values(df: pd.DataFrame, name: str = \"frame\") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Create canonical helper columns, coerce numerics, and print a validation summary.\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # target labels and numeric target\n",
    "    out[\"_case_label\"] = canon_case_status(out[\"case_status\"])\n",
    "    out[\"y\"] = out[\"_case_label\"].map({\"Certified\":1, \"Denied\":0}).astype(\"Int64\")\n",
    "\n",
    "    # Y/N flags\n",
    "    out[\"_job_exp\"]   = canon_yn(out[\"has_job_experience\"])\n",
    "    out[\"_job_train\"] = canon_yn(out[\"requires_job_training\"])\n",
    "    out[\"_full_time\"] = canon_yn(out[\"full_time_position\"])\n",
    "\n",
    "    # categoricals\n",
    "    out[\"_continent\"] = canon_title(out[\"continent\"])\n",
    "    out[\"_region\"]    = canon_title(out[\"region_of_employment\"])\n",
    "    out[\"_uow\"]       = canon_title(out[\"unit_of_wage\"])\n",
    "    out[\"_edu\"]       = canon_education(out[\"education_of_employee\"])\n",
    "\n",
    "    # numerics keep raw wage normalization \n",
    "    out[\"no_of_employees\"] = pd.to_numeric(out[\"no_of_employees\"], errors=\"coerce\")\n",
    "    out[\"yr_of_estab\"]     = pd.to_numeric(out[\"yr_of_estab\"], errors=\"coerce\")\n",
    "    out[\"prevailing_wage\"] = pd.to_numeric(out[\"prevailing_wage\"], errors=\"coerce\")\n",
    "\n",
    "    # Validation report \n",
    "    # Validation report \n",
    "    rep: Dict[str, Any] = {\"dataset\": name}\n",
    "\n",
    "    # target check\n",
    "    lbl_counts = out[\"_case_label\"].value_counts(dropna=False).to_dict()\n",
    "    y_counts   = out[\"y\"].value_counts(dropna=False).to_dict()\n",
    "    rep[\"target_label_counts\"]  = lbl_counts\n",
    "    rep[\"target_numeric_counts\"] = y_counts\n",
    "    rep[\"target_na\"] = int(out[\"_case_label\"].isna().sum())\n",
    "\n",
    "    # Y/N validity\n",
    "    def yn_invalid(col: str) -> int:\n",
    "        # True where value is NOT in {Y,N} or is NA\n",
    "        mask_valid = out[col].isin([\"Y\",\"N\"])\n",
    "        invalid = (~mask_valid) | mask_valid.isna()\n",
    "        return int(invalid.sum())\n",
    "\n",
    "    rep[\"job_exp_invalid\"]   = yn_invalid(\"_job_exp\")\n",
    "    rep[\"job_train_invalid\"] = yn_invalid(\"_job_train\")\n",
    "    rep[\"full_time_invalid\"] = yn_invalid(\"_full_time\")\n",
    "\n",
    "    # education distribution flag 'Other' share if present\n",
    "    edu_counts = out[\"_edu\"].value_counts(dropna=False)\n",
    "    rep[\"edu_counts\"] = edu_counts.to_dict()\n",
    "    rep[\"edu_other_pct\"] = float((edu_counts.get(\"Other\", 0) / max(1, edu_counts.sum())) * 100)\n",
    "\n",
    "    # Print concise audit\n",
    "    print(f\"[{name}] TARGET label counts:\", lbl_counts)\n",
    "    print(f\"[{name}] TARGET numeric counts:\", y_counts)\n",
    "    print(f\"[{name}] TARGET NaNs: {rep['target_na']}\")\n",
    "    print(f\"[{name}] YN invalid → job_exp={rep['job_exp_invalid']} | job_train={rep['job_train_invalid']} | full_time={rep['full_time_invalid']}\")\n",
    "    print(f\"[{name}] Education 'Other' share: {rep['edu_other_pct']:.1f}%\")\n",
    "\n",
    "    # pass criteria no target NaNs & YN all valid\n",
    "    rep[\"passed\"] = (rep[\"target_na\"] == 0 and \n",
    "                    rep[\"job_exp_invalid\"] == 0 and \n",
    "                    rep[\"job_train_invalid\"] == 0 and \n",
    "                    rep[\"full_time_invalid\"] == 0)\n",
    "\n",
    "    status = \"PASS ✓\" if rep[\"passed\"] else \"WARN ✗\"\n",
    "    print(f\"[{name}] VALUE SUMMARY ⇒ {status}\")\n",
    "    return out, rep\n",
    "\n",
    "# One call convenience\n",
    "def canonicalize_and_validate(df: pd.DataFrame, name: str = \"EasyVisa\", strict: bool = False) -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Column rename + value canonicalization with full validation.\n",
    "    Returns (DF, column_report, value_report).\n",
    "    \"\"\"\n",
    "    df1, col_rep = canonicalize_columns(df, name=f\"{name}-cols\")\n",
    "    DF, val_rep  = apply_canonical_values(df1, name=f\"{name}-vals\")\n",
    "\n",
    "    all_pass = col_rep[\"passed\"] and val_rep[\"passed\"]\n",
    "    final_status = \"PASS ✓\" if all_pass else \"WARN ✗\"\n",
    "    print(f\"[{name}] CANONICALIZATION SUMMARY → columns={col_rep['passed']} & values={val_rep['passed']} ⇒ {final_status}\\n\")\n",
    "\n",
    "    if strict and not all_pass:\n",
    "        raise AssertionError(f\"[{name}] Canonicalization failed: col={col_rep}, val={val_rep}\")\n",
    "\n",
    "    return DF, col_rep, val_rep\n",
    "\n",
    "# Build canonical frame with verification\n",
    "DF, col_report, val_report = canonicalize_and_validate(df, name=\"EasyVisa\", strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88b8eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EasyVisa-cols] No unknown headers. ✓\n",
      "[EasyVisa-cols] MISSING expected columns: None ✓\n",
      "[EasyVisa-cols] No collisions. ✓\n",
      "[EasyVisa-cols] SUMMARY → unknown=0 | missing=0 | collisions=0  ⇒  PASS ✓\n"
     ]
    }
   ],
   "source": [
    "# C) RENAME AND SCHEMA GUARD\n",
    "from typing import Dict, Any\n",
    "import difflib\n",
    "\n",
    "# helper/internal columns create during canonicalization\n",
    "INTERNAL_COLS = {\n",
    "    \"_case_label\",\"_edu\",\"_continent\",\"_region\",\"_uow\",\n",
    "    \"_job_exp\",\"_job_train\",\"_full_time\"\n",
    "}\n",
    "\n",
    "def rename_canonical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename columns using _CANON_MAP while avoiding alias collisions.\"\"\"\n",
    "    ren, used = {}, set()\n",
    "    for c in df.columns:\n",
    "        k = _norm(c)\n",
    "        new = _CANON_MAP.get(k, c)\n",
    "        # avoid overwriting when alias and real both exist\n",
    "        if new in used and new != c:\n",
    "            new = c\n",
    "        ren[c] = new\n",
    "        used.add(new)\n",
    "    return df.rename(columns=ren).copy()\n",
    "\n",
    "def schema_guard(df: pd.DataFrame, name: str = \"frame\", strict: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate schema quality:\n",
    "      - unknown headers (not in alias map or expected), ignoring internal helper cols\n",
    "      - missing expected columns (after rename)\n",
    "      - collisions (two raw headers normalize to same key)\n",
    "\n",
    "    Returns a dict report; raises AssertionError if strict=True and validation fails.\n",
    "    \"\"\"\n",
    "    raw  = list(df.columns)\n",
    "    keys = [_norm(c) for c in raw]\n",
    "\n",
    "    # Unknown headers skip internals and underscore-prefixed helpers \n",
    "    unknown = [\n",
    "        (r, k) for r, k in zip(raw, keys)\n",
    "        if (r not in INTERNAL_COLS)\n",
    "        and (not str(r).startswith(\"_\"))\n",
    "        and (k not in _CANON_MAP)\n",
    "        and (r not in EXPECTED_COLS)\n",
    "    ]\n",
    "    if unknown:\n",
    "        print(f\"[{name}] UNKNOWN headers:\", [r for r,_ in unknown])\n",
    "        print(f\"[{name}] Suggested _CANON_MAP patches:\")\n",
    "        for r, k in unknown:\n",
    "            guess = difflib.get_close_matches(k, list(_CANON_MAP.keys()), n=1, cutoff=0.6)\n",
    "            guess_str = guess[0] if guess else \"<add-key>\"\n",
    "            print(f'    _CANON_MAP[\"{k}\"] = \"{r}\"   # raw=\"{r}\"')\n",
    "    else:\n",
    "        print(f\"[{name}] No unknown headers. ✓\")\n",
    "\n",
    "    # Missing expected columns after rename\n",
    "    missing = sorted([c for c in EXPECTED_COLS if c not in df.columns])\n",
    "    print(f\"[{name}] MISSING expected columns:\", missing if missing else \"None ✓\")\n",
    "\n",
    "    # Collisions two raw → same normalized key\n",
    "    seen, collisions = {}, {}\n",
    "    for r, k in zip(raw, keys):\n",
    "        if k in seen and seen[k] != r:\n",
    "            collisions.setdefault(k, set()).update([seen[k], r])\n",
    "        else:\n",
    "            seen[k] = r\n",
    "    if collisions:\n",
    "        print(f\"[{name}] COLLISIONS:\")\n",
    "        for k, raws in collisions.items():\n",
    "            print(f\"   norm='{k}' ← raw {sorted(list(raws))}\")\n",
    "    else:\n",
    "        print(f\"[{name}] No collisions. ✓\")\n",
    "\n",
    "    # Summary and machine readable report \n",
    "    report = {\n",
    "        \"dataset\": name,\n",
    "        \"unknown\": [r for r,_ in unknown],\n",
    "        \"unknown_count\": len(unknown),\n",
    "        \"missing\": missing,\n",
    "        \"missing_count\": len(missing),\n",
    "        \"collisions\": {k: sorted(list(v)) for k, v in collisions.items()},\n",
    "        \"collision_count\": len(collisions),\n",
    "        \"passed\": (len(unknown) == 0 and len(missing) == 0 and len(collisions) == 0),\n",
    "    }\n",
    "    status = \"PASS ✓\" if report[\"passed\"] else \"FAIL ✗\"\n",
    "    print(f\"[{name}] SUMMARY → unknown={report['unknown_count']} | \"\n",
    "          f\"missing={report['missing_count']} | collisions={report['collision_count']}  ⇒  {status}\")\n",
    "\n",
    "    if strict and not report[\"passed\"]:\n",
    "        raise AssertionError(f\"[{name}] Schema validation failed: {report}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "# After loading raw df\n",
    "df_renamed = rename_canonical(df)\n",
    "\n",
    "# Print audit and get structured report\n",
    "rep = schema_guard(df_renamed, name=\"EasyVisa-cols\", strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da6c9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D) BUILD CANONICAL DF\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "def build_canonical_df(df: pd.DataFrame, name: str = \"EasyVisa\", strict: bool = False\n",
    "                      ) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rename → canonicalize values → coerce numerics → validate.\n",
    "    Returns (DF, report). If strict=True, raises on failed validation.\n",
    "    \"\"\"\n",
    "    # 1) Column rename and schema audit\n",
    "    out = rename_canonical(df).copy()\n",
    "    col_rep = schema_guard(out, f\"{name}-cols\", strict=False)\n",
    "\n",
    "    # 2) Target string and numeric\n",
    "    out[TARGET_LBL] = canon_case_status(out[TARGET_RAW])\n",
    "    out[TARGET_BIN] = out[TARGET_LBL].map({\"Certified\": 1, \"Denied\": 0}).astype(\"Int64\")\n",
    "\n",
    "    # 3) Binary flags\n",
    "    out[\"_job_exp\"]   = canon_yn(out[\"has_job_experience\"])\n",
    "    out[\"_job_train\"] = canon_yn(out[\"requires_job_training\"])\n",
    "    out[\"_full_time\"] = canon_yn(out[\"full_time_position\"])\n",
    "\n",
    "    # 4) Core categoricals\n",
    "    out[\"_continent\"] = canon_title(out[\"continent\"])\n",
    "    out[\"_region\"]    = canon_title(out[\"region_of_employment\"])\n",
    "    out[\"_uow\"]       = canon_title(out[\"unit_of_wage\"])\n",
    "    out[\"_edu\"]       = canon_education(out[\"education_of_employee\"])\n",
    "\n",
    "    # 5) Numerics coerce safely wage normalization happens later\n",
    "    out[\"no_of_employees\"] = pd.to_numeric(out[\"no_of_employees\"], errors=\"coerce\")\n",
    "    out[\"yr_of_estab\"]     = pd.to_numeric(out[\"yr_of_estab\"], errors=\"coerce\")\n",
    "    out[\"prevailing_wage\"] = pd.to_numeric(out[\"prevailing_wage\"], errors=\"coerce\")\n",
    "\n",
    "    # 6) Value level validation\n",
    "    rep: Dict[str, Any] = {\"dataset\": name}\n",
    "\n",
    "    # Target checks\n",
    "    rep[\"target_label_counts\"]  = out[TARGET_LBL].value_counts(dropna=False).to_dict()\n",
    "    rep[\"target_numeric_counts\"] = out[TARGET_BIN].value_counts(dropna=False).to_dict()\n",
    "    rep[\"target_na\"] = int(out[TARGET_LBL].isna().sum())\n",
    "    rep[\"y_na\"]      = int(out[TARGET_BIN].isna().sum())\n",
    "\n",
    "    # Y/N validity\n",
    "    def yn_invalid(col: str) -> int:\n",
    "        m = out[col].isin([\"Y\", \"N\"])\n",
    "        return int((~m | m.isna()).sum())\n",
    "\n",
    "    rep[\"job_exp_invalid\"]   = yn_invalid(\"_job_exp\")\n",
    "    rep[\"job_train_invalid\"] = yn_invalid(\"_job_train\")\n",
    "    rep[\"full_time_invalid\"] = yn_invalid(\"_full_time\")\n",
    "\n",
    "    # Numeric NA counts sanity only\n",
    "    rep[\"no_of_employees_na\"] = int(out[\"no_of_employees\"].isna().sum())\n",
    "    rep[\"yr_of_estab_na\"]     = int(out[\"yr_of_estab\"].isna().sum())\n",
    "    rep[\"prevailing_wage_na\"] = int(out[\"prevailing_wage\"].isna().sum())\n",
    "\n",
    "    # Pass criteria values — tune as needed\n",
    "    rep[\"values_passed\"] = (\n",
    "        rep[\"target_na\"] == 0 and rep[\"y_na\"] == 0\n",
    "        and rep[\"job_exp_invalid\"] == 0\n",
    "        and rep[\"job_train_invalid\"] == 0\n",
    "        and rep[\"full_time_invalid\"] == 0\n",
    "    )\n",
    "\n",
    "    # 7) Final summary\n",
    "    overall_pass = col_rep[\"passed\"] and rep[\"values_passed\"]\n",
    "    rep[\"columns_passed\"] = col_rep[\"passed\"]\n",
    "    rep[\"passed\"] = overall_pass\n",
    "\n",
    "    # Print concise summary\n",
    "    print(f\"[{name}-vals] TARGET NaNs: labels={rep['target_na']} | y={rep['y_na']}\")\n",
    "    print(f\"[{name}-vals] Y/N invalid → job_exp={rep['job_exp_invalid']} | \"\n",
    "          f\"job_train={rep['job_train_invalid']} | full_time={rep['full_time_invalid']}\")\n",
    "    print(f\"[{name}-vals] Numeric NaNs → employees={rep['no_of_employees_na']} | \"\n",
    "          f\"yr_of_estab={rep['yr_of_estab_na']} | wage={rep['prevailing_wage_na']}\")\n",
    "\n",
    "    status_cols = \"PASS ✓\" if col_rep[\"passed\"]     else \"FAIL ✗\"\n",
    "    status_vals = \"PASS ✓\" if rep[\"values_passed\"]  else \"FAIL ✗\"\n",
    "    status_all  = \"PASS ✓\" if overall_pass          else \"FAIL ✗\"\n",
    "    print(f\"[{name}] CANON SUMMARY → columns={status_cols} | values={status_vals}  ⇒  {status_all}\\n\")\n",
    "\n",
    "    if strict and not overall_pass:\n",
    "        raise AssertionError(f\"[{name}] Canonicalization failed: columns={col_rep}, values={rep}\")\n",
    "\n",
    "    return out, rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26247c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RNG reproducibility\n",
      "[OK] Target column present\n",
      "[OK] All expected columns present\n",
      "[INFO] Extra columns (not required): ['_case_label', '_continent', '_edu', '_full_time', '_job_exp', '_job_train', '_region', '_uow', 'y']\n",
      "[EasyVisa-cols] No unknown headers. ✓\n",
      "[EasyVisa-cols] MISSING expected columns: None ✓\n",
      "[EasyVisa-cols] No collisions. ✓\n",
      "[EasyVisa-cols] SUMMARY → unknown=0 | missing=0 | collisions=0  ⇒  PASS ✓\n",
      "[EasyVisa-vals] TARGET NaNs: labels=0 | y=0\n",
      "[EasyVisa-vals] Y/N invalid → job_exp=0 | job_train=0 | full_time=0\n",
      "[EasyVisa-vals] Numeric NaNs → employees=0 | yr_of_estab=0 | wage=0\n",
      "[EasyVisa] CANON SUMMARY → columns=PASS ✓ | values=PASS ✓  ⇒  PASS ✓\n",
      "\n",
      "[INFO] target label balance →\n",
      "_case_label\n",
      "Certified    17018\n",
      "Denied        8462\n",
      "  Imbalance ratio ≈ 2.01:1\n",
      "[INFO] target numeric balance →\n",
      "y\n",
      "1    17018\n",
      "0     8462\n",
      "  Imbalance ratio ≈ 2.01:1\n",
      "\n",
      "[INFO] canonical categorical uniques: {'_edu': 4, '_continent': 6, '_region': 5, '_uow': 4, '_job_exp': 2, '_job_train': 2, '_full_time': 2}\n",
      "\n",
      "================ SUMMARY ================\n",
      "rows=25,480  cols=21  |  raw_expected_ok=True  columns_passed=True  values_passed=True  ⇒  PASS ✓\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E) RUN & REPORT \n",
    "# 1) Core verifications on the raw frame\n",
    "verify_rng()\n",
    "verify_target_exists(DF)\n",
    "raw_schema = verify_expected_columns(DF)\n",
    "\n",
    "# 2) Build canonical dataframe and value audit\n",
    "DF, canon_report = build_canonical_df(df, name=\"EasyVisa\", strict=False)\n",
    "\n",
    "# 3) Target class balance labels and numeric\n",
    "lbl_bal = report_class_balance(DF[TARGET_LBL], \"[INFO] target label balance\")\n",
    "num_bal = report_class_balance(DF[TARGET_BIN], \"[INFO] target numeric balance\")\n",
    "\n",
    "# 4) Quick canonical feature sanity uniques\n",
    "key_cats = [\"_edu\",\"_continent\",\"_region\",\"_uow\",\"_job_exp\",\"_job_train\",\"_full_time\"]\n",
    "uniques = {c: int(DF[c].dropna().nunique()) for c in key_cats}\n",
    "print(\"\\n[INFO] canonical categorical uniques:\", uniques)\n",
    "\n",
    "# 5) Compact PASS / FAIL footer\n",
    "overall_pass = bool(raw_schema[\"passed\"] and canon_report[\"passed\"])\n",
    "status = \"PASS ✓\" if overall_pass else \"WARN ✗\"\n",
    "print(\"\\n================ SUMMARY ================\")\n",
    "print(f\"rows={len(DF):,}  cols={DF.shape[1]}  |  \"\n",
    "      f\"raw_expected_ok={raw_schema['passed']}  \"\n",
    "      f\"columns_passed={canon_report['columns_passed']}  \"\n",
    "      f\"values_passed={canon_report['values_passed']}  ⇒  {status}\")\n",
    "print(\"========================================\\n\")\n",
    "\n",
    "# 6) Machine readable bundle \n",
    "verification_bundle = {\n",
    "    \"raw_schema\": raw_schema,\n",
    "    \"canonicalization\": canon_report,\n",
    "    \"label_balance\": lbl_bal,\n",
    "    \"numeric_balance\": num_bal,\n",
    "    \"cat_uniques\": uniques,\n",
    "    \"passed\": overall_pass,\n",
    "}\n",
    "\n",
    "assert verification_bundle[\"passed\"], f\"Verification failed: {verification_bundle}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-venue",
   "metadata": {
    "id": "american-venue"
   },
   "source": [
    "#### Let's check the statistical summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "PsJ9MaHRN4U5",
   "metadata": {
    "id": "PsJ9MaHRN4U5"
   },
   "outputs": [],
   "source": [
    "# 1) Total missing values\n",
    "DF.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# 2) Split by type\n",
    "DF.select_dtypes('number').isna().sum().sort_values(ascending=False)\n",
    "DF.select_dtypes(exclude='number').isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Hard guardrail\n",
    "assert DF.isna().sum().sum() == 0, \"Unexpected NaNs remain after cleaning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "659a9f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   count          mean           std     min       25%  \\\n",
       " no_of_employees  25480.0    5667.04321  22877.928848   -26.0    1022.0   \n",
       " yr_of_estab      25480.0   1979.409929     42.366929  1800.0    1976.0   \n",
       " prevailing_wage  25480.0  74455.814592  52815.942327  2.1367  34015.48   \n",
       " y                25480.0      0.667896      0.470977     0.0       0.0   \n",
       " \n",
       "                       50%          75%        max  nulls  null_pct  \n",
       " no_of_employees    2109.0       3504.0   602069.0      0       0.0  \n",
       " yr_of_estab        1997.0       2005.0     2016.0      0       0.0  \n",
       " prevailing_wage  70308.21  107735.5125  319210.27      0       0.0  \n",
       " y                     1.0          1.0        1.0      0       0.0  ,\n",
       "                        count unique         top   freq  nulls  null_pct\n",
       " case_id                25480  25480    EZYV8640      1      0       0.0\n",
       " continent              25480      6        Asia  16861      0       0.0\n",
       " education_of_employee  25480      4  Bachelor's  10234      0       0.0\n",
       " has_job_experience     25480      2           Y  14802      0       0.0\n",
       " requires_job_training  25480      2           N  22525      0       0.0\n",
       " region_of_employment   25480      5   Northeast   7195      0       0.0\n",
       " unit_of_wage           25480      4        Year  22962      0       0.0\n",
       " full_time_position     25480      2           Y  22773      0       0.0\n",
       " case_status            25480      2   Certified  17018      0       0.0\n",
       " _case_label            25480      2   Certified  17018      0       0.0\n",
       " _job_exp               25480      2           Y  14802      0       0.0\n",
       " _job_train             25480      2           N  22525      0       0.0\n",
       " _full_time             25480      2           Y  22773      0       0.0\n",
       " _continent             25480      6        Asia  16861      0       0.0\n",
       " _region                25480      5   Northeast   7195      0       0.0\n",
       " _uow                   25480      4        Year  22962      0       0.0\n",
       " _edu                   25480      4  Bachelor's  10234      0       0.0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numeric summary\n",
    "num_cols = DF.select_dtypes('number').columns\n",
    "num_desc = DF[num_cols].describe().T\n",
    "num_desc['nulls'] = DF[num_cols].isna().sum().values\n",
    "num_desc['null_pct'] = (DF[num_cols].isna().mean().values * 100).round(2)\n",
    "\n",
    "# Categorical summary\n",
    "cat_cols = DF.select_dtypes(exclude='number').columns\n",
    "cat_desc = DF[cat_cols].describe().T  \n",
    "cat_desc['nulls'] = DF[cat_cols].isna().sum().values\n",
    "cat_desc['null_pct'] = (DF[cat_cols].isna().mean().values * 100).round(2)\n",
    "\n",
    "num_desc, cat_desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d086b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <td>25480</td>\n",
       "      <td>25480</td>\n",
       "      <td>EZYV8640</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continent</th>\n",
       "      <td>25480</td>\n",
       "      <td>6</td>\n",
       "      <td>Asia</td>\n",
       "      <td>16861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_of_employee</th>\n",
       "      <td>25480</td>\n",
       "      <td>4</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>10234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_job_experience</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>14802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requires_job_training</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>22525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_employees</th>\n",
       "      <td>25480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5667.04321</td>\n",
       "      <td>22877.928848</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>602069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr_of_estab</th>\n",
       "      <td>25480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.409929</td>\n",
       "      <td>42.366929</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_of_employment</th>\n",
       "      <td>25480</td>\n",
       "      <td>5</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>7195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevailing_wage</th>\n",
       "      <td>25480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74455.814592</td>\n",
       "      <td>52815.942327</td>\n",
       "      <td>2.1367</td>\n",
       "      <td>34015.48</td>\n",
       "      <td>70308.21</td>\n",
       "      <td>107735.5125</td>\n",
       "      <td>319210.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_of_wage</th>\n",
       "      <td>25480</td>\n",
       "      <td>4</td>\n",
       "      <td>Year</td>\n",
       "      <td>22962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_time_position</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>22773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_status</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Certified</td>\n",
       "      <td>17018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_case_label</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Certified</td>\n",
       "      <td>17018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>25480.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.667896</td>\n",
       "      <td>0.470977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_job_exp</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>14802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_job_train</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>22525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_full_time</th>\n",
       "      <td>25480</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>22773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_continent</th>\n",
       "      <td>25480</td>\n",
       "      <td>6</td>\n",
       "      <td>Asia</td>\n",
       "      <td>16861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_region</th>\n",
       "      <td>25480</td>\n",
       "      <td>5</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>7195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_uow</th>\n",
       "      <td>25480</td>\n",
       "      <td>4</td>\n",
       "      <td>Year</td>\n",
       "      <td>22962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_edu</th>\n",
       "      <td>25480</td>\n",
       "      <td>4</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>10234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count unique         top   freq          mean  \\\n",
       "case_id                  25480  25480    EZYV8640      1           NaN   \n",
       "continent                25480      6        Asia  16861           NaN   \n",
       "education_of_employee    25480      4  Bachelor's  10234           NaN   \n",
       "has_job_experience       25480      2           Y  14802           NaN   \n",
       "requires_job_training    25480      2           N  22525           NaN   \n",
       "no_of_employees        25480.0    NaN         NaN    NaN    5667.04321   \n",
       "yr_of_estab            25480.0    NaN         NaN    NaN   1979.409929   \n",
       "region_of_employment     25480      5   Northeast   7195           NaN   \n",
       "prevailing_wage        25480.0    NaN         NaN    NaN  74455.814592   \n",
       "unit_of_wage             25480      4        Year  22962           NaN   \n",
       "full_time_position       25480      2           Y  22773           NaN   \n",
       "case_status              25480      2   Certified  17018           NaN   \n",
       "_case_label              25480      2   Certified  17018           NaN   \n",
       "y                      25480.0   <NA>        <NA>   <NA>      0.667896   \n",
       "_job_exp                 25480      2           Y  14802           NaN   \n",
       "_job_train               25480      2           N  22525           NaN   \n",
       "_full_time               25480      2           Y  22773           NaN   \n",
       "_continent               25480      6        Asia  16861           NaN   \n",
       "_region                  25480      5   Northeast   7195           NaN   \n",
       "_uow                     25480      4        Year  22962           NaN   \n",
       "_edu                     25480      4  Bachelor's  10234           NaN   \n",
       "\n",
       "                                std     min       25%       50%          75%  \\\n",
       "case_id                         NaN     NaN       NaN       NaN          NaN   \n",
       "continent                       NaN     NaN       NaN       NaN          NaN   \n",
       "education_of_employee           NaN     NaN       NaN       NaN          NaN   \n",
       "has_job_experience              NaN     NaN       NaN       NaN          NaN   \n",
       "requires_job_training           NaN     NaN       NaN       NaN          NaN   \n",
       "no_of_employees        22877.928848   -26.0    1022.0    2109.0       3504.0   \n",
       "yr_of_estab               42.366929  1800.0    1976.0    1997.0       2005.0   \n",
       "region_of_employment            NaN     NaN       NaN       NaN          NaN   \n",
       "prevailing_wage        52815.942327  2.1367  34015.48  70308.21  107735.5125   \n",
       "unit_of_wage                    NaN     NaN       NaN       NaN          NaN   \n",
       "full_time_position              NaN     NaN       NaN       NaN          NaN   \n",
       "case_status                     NaN     NaN       NaN       NaN          NaN   \n",
       "_case_label                     NaN     NaN       NaN       NaN          NaN   \n",
       "y                          0.470977     0.0       0.0       1.0          1.0   \n",
       "_job_exp                        NaN     NaN       NaN       NaN          NaN   \n",
       "_job_train                      NaN     NaN       NaN       NaN          NaN   \n",
       "_full_time                      NaN     NaN       NaN       NaN          NaN   \n",
       "_continent                      NaN     NaN       NaN       NaN          NaN   \n",
       "_region                         NaN     NaN       NaN       NaN          NaN   \n",
       "_uow                            NaN     NaN       NaN       NaN          NaN   \n",
       "_edu                            NaN     NaN       NaN       NaN          NaN   \n",
       "\n",
       "                             max  \n",
       "case_id                      NaN  \n",
       "continent                    NaN  \n",
       "education_of_employee        NaN  \n",
       "has_job_experience           NaN  \n",
       "requires_job_training        NaN  \n",
       "no_of_employees         602069.0  \n",
       "yr_of_estab               2016.0  \n",
       "region_of_employment         NaN  \n",
       "prevailing_wage        319210.27  \n",
       "unit_of_wage                 NaN  \n",
       "full_time_position           NaN  \n",
       "case_status                  NaN  \n",
       "_case_label                  NaN  \n",
       "y                            1.0  \n",
       "_job_exp                     NaN  \n",
       "_job_train                   NaN  \n",
       "_full_time                   NaN  \n",
       "_continent                   NaN  \n",
       "_region                      NaN  \n",
       "_uow                         NaN  \n",
       "_edu                         NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the statistics\n",
    "DF.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-timing",
   "metadata": {
    "id": "competent-timing"
   },
   "source": [
    "#### Fixing the negative values in number of employees columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ltjRQiBN40d",
   "metadata": {
    "id": "_ltjRQiBN40d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cutting-bookmark",
   "metadata": {
    "id": "cutting-bookmark"
   },
   "source": [
    "#### Let's check the count of each unique category in each of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tjT97Rc9N5SC",
   "metadata": {
    "id": "tjT97Rc9N5SC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wooden-christian",
   "metadata": {
    "id": "wooden-christian"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "superb-springfield",
   "metadata": {
    "id": "superb-springfield"
   },
   "outputs": [],
   "source": [
    "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (15,10))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "R59VWoa5Wdbx",
   "metadata": {
    "id": "R59VWoa5Wdbx"
   },
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-command",
   "metadata": {
    "id": "editorial-command"
   },
   "source": [
    "#### Observations on education of employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8PjkMDRiN1lA",
   "metadata": {
    "id": "8PjkMDRiN1lA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attempted-burlington",
   "metadata": {
    "id": "attempted-burlington"
   },
   "source": [
    "#### Observations on region of employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_eEpcxf4N2GY",
   "metadata": {
    "id": "_eEpcxf4N2GY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-kidney",
   "metadata": {
    "id": "forbidden-kidney"
   },
   "source": [
    "#### Observations on job experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W2OyS2efN2mw",
   "metadata": {
    "id": "W2OyS2efN2mw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stunning-surrey",
   "metadata": {
    "id": "stunning-surrey"
   },
   "source": [
    "#### Observations on case status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0kaXC-PhN3IU",
   "metadata": {
    "id": "0kaXC-PhN3IU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-aging",
   "metadata": {
    "id": "equivalent-aging"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-convertible",
   "metadata": {
    "id": "blond-convertible"
   },
   "source": [
    "**Creating functions that will help us with further analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adaptive-recipient",
   "metadata": {
    "id": "adaptive-recipient"
   },
   "outputs": [],
   "source": [
    "### function to plot distributions wrt target\n",
    "\n",
    "\n",
    "def distribution_plot_wrt_target(data, predictor, target):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    target_uniq = data[target].unique()\n",
    "\n",
    "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[0]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 0],\n",
    "        color=\"teal\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[1]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 1],\n",
    "        color=\"orange\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
    "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
    "\n",
    "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x=target,\n",
    "        y=predictor,\n",
    "        ax=axs[1, 1],\n",
    "        showfliers=False,\n",
    "        palette=\"gist_rainbow\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "third-sheriff",
   "metadata": {
    "id": "third-sheriff"
   },
   "outputs": [],
   "source": [
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\", frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-excuse",
   "metadata": {
    "id": "dressed-excuse"
   },
   "source": [
    "#### Does higher education increase the chances of visa certification for well-paid jobs abroad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VtZ978lDNxfu",
   "metadata": {
    "id": "VtZ978lDNxfu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attended-current",
   "metadata": {
    "id": "attended-current"
   },
   "source": [
    "#### How does visa status vary across different continents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evbNlj4XNyBe",
   "metadata": {
    "id": "evbNlj4XNyBe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "macro-decrease",
   "metadata": {
    "id": "macro-decrease"
   },
   "source": [
    "#### Does having prior work experience influence the chances of visa certification for career opportunities abroad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcf_oGNyfo",
   "metadata": {
    "id": "d3fcf_oGNyfo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "changing-kansas",
   "metadata": {
    "id": "changing-kansas"
   },
   "source": [
    "#### Is the prevailing wage consistent across all regions of the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0FYdlpG5NzEe",
   "metadata": {
    "id": "0FYdlpG5NzEe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lesser-bacteria",
   "metadata": {
    "id": "lesser-bacteria"
   },
   "source": [
    "#### Does visa status vary with changes in the prevailing wage set to protect both local talent and foreign workers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HIr2VsTGNzhA",
   "metadata": {
    "id": "HIr2VsTGNzhA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suspected-asthma",
   "metadata": {
    "id": "suspected-asthma"
   },
   "source": [
    "#### Does the unit of prevailing wage (Hourly, Weekly, etc.) have any impact on the likelihood of visa application certification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KgXcBCf0N0IA",
   "metadata": {
    "id": "KgXcBCf0N0IA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qBWlk20UBUAx",
   "metadata": {
    "id": "qBWlk20UBUAx"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-association",
   "metadata": {
    "id": "allied-association"
   },
   "source": [
    "### Outlier Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H76eABeDCJWY",
   "metadata": {
    "id": "H76eABeDCJWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "flexible-independence",
   "metadata": {
    "id": "flexible-independence"
   },
   "source": [
    "### Data Preparation for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YsRIntveCJ9_",
   "metadata": {
    "id": "YsRIntveCJ9_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dr7q6-dbfiQB",
   "metadata": {
    "id": "dr7q6-dbfiQB"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrlw9AVcqk37",
   "metadata": {
    "id": "rrlw9AVcqk37"
   },
   "source": [
    "### Model Evaluation Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vw4vUNvIql3X",
   "metadata": {
    "id": "vw4vUNvIql3X"
   },
   "source": [
    "- Choose the primary metric to evaluate the model on\n",
    "- Elaborate on the rationale behind choosing the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PepZef6UqpNa",
   "metadata": {
    "id": "PepZef6UqpNa"
   },
   "source": [
    "First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
    "* The `model_performance_classification_sklearn` function will be used to check the model performance of models.\n",
    "* The `confusion_matrix_sklearn` function will be used to plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "mexican-database",
   "metadata": {
    "id": "mexican-database"
   },
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "\n",
    "\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "recreational-topic",
   "metadata": {
    "id": "recreational-topic"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0QZZoxoDcoDm",
   "metadata": {
    "id": "0QZZoxoDcoDm"
   },
   "source": [
    "#### Defining scorer to be used for cross-validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XWoHuUpjbp0_",
   "metadata": {
    "id": "XWoHuUpjbp0_"
   },
   "source": [
    "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fI98GOV0pTY",
   "metadata": {
    "id": "4fI98GOV0pTY"
   },
   "source": [
    "### Model building with Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vOPuYn7dCGjx",
   "metadata": {
    "id": "vOPuYn7dCGjx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "C91_6Swtbp1A",
   "metadata": {
    "id": "C91_6Swtbp1A"
   },
   "source": [
    "### Model Building with Oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PzWwDZrwCHJJ",
   "metadata": {
    "id": "PzWwDZrwCHJJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fYLfDmHvbp1B",
   "metadata": {
    "id": "fYLfDmHvbp1B"
   },
   "source": [
    "### Model Building with Undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LAvFgREaCHqm",
   "metadata": {
    "id": "LAvFgREaCHqm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Cg_OREBD1NOy",
   "metadata": {
    "id": "Cg_OREBD1NOy"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nVuzJqcU8cd0",
   "metadata": {
    "id": "nVuzJqcU8cd0"
   },
   "source": [
    "**Best practices for hyperparameter tuning in AdaBoost:**\n",
    "\n",
    "`n_estimators`:\n",
    "\n",
    "- Start with a specific number (50 is used in general) and increase in steps: 50, 75, 85, 100\n",
    "\n",
    "- Use fewer estimators (e.g., 50 to 100) if using complex base learners (like deeper decision trees)\n",
    "\n",
    "- Use more estimators (e.g., 100 to 150) when learning rate is low (e.g., 0.1 or lower)\n",
    "\n",
    "- Avoid very high values unless performance keeps improving on validation\n",
    "\n",
    "`learning_rate`:\n",
    "\n",
    "- Common values to try: 1.0, 0.5, 0.1, 0.01\n",
    "\n",
    "- Use 1.0 for faster training, suitable for fewer estimators\n",
    "\n",
    "- Use 0.1 or 0.01 when using more estimators to improve generalization\n",
    "\n",
    "- Avoid very small values (< 0.01) unless you plan to use many estimators (e.g., >500) and have sufficient data\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cciBaj3v8dSs",
   "metadata": {
    "id": "cciBaj3v8dSs"
   },
   "source": [
    "**Best practices for hyperparameter tuning in Random Forest:**\n",
    "\n",
    "\n",
    "`n_estimators`:\n",
    "\n",
    "* Start with a specific number (50 is used in general) and increase in steps: 50, 75, 100, 125\n",
    "* Higher values generally improve performance but increase training time\n",
    "* Use 100-150 for large datasets or when variance is high\n",
    "\n",
    "\n",
    "`min_samples_leaf`:\n",
    "\n",
    "* Try values like: 1, 2, 4, 5, 10\n",
    "* Higher values reduce model complexity and help prevent overfitting\n",
    "* Use 1–2 for low-bias models, higher (like 5 or 10) for more regularized models\n",
    "* Works well in noisy datasets to smooth predictions\n",
    "\n",
    "\n",
    "`max_features`:\n",
    "\n",
    "* Try values: `\"sqrt\"` (default for classification), `\"log2\"`, `None`, or float values (e.g., `0.3`, `0.5`)\n",
    "* `\"sqrt\"` balances between diversity and performance for classification tasks\n",
    "* Lower values (e.g., `0.3`) increase tree diversity, reducing overfitting\n",
    "* Higher values (closer to `1.0`) may capture more interactions but risk overfitting\n",
    "\n",
    "\n",
    "`max_samples` (for bootstrap sampling):\n",
    "\n",
    "* Try float values between `0.5` to `1.0` or fixed integers\n",
    "* Use `0.6–0.9` to introduce randomness and reduce overfitting\n",
    "* Smaller values increase diversity between trees, improving generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EvIBMJe-8gEm",
   "metadata": {
    "id": "EvIBMJe-8gEm"
   },
   "source": [
    "**Best practices for hyperparameter tuning in Gradient Boosting:**\n",
    "\n",
    "`n_estimators`:\n",
    "\n",
    "* Start with 100 (default) and increase: 100, 200, 300, 500\n",
    "* Typically, higher values lead to better performance, but they also increase training time\n",
    "* Use 200–500 for larger datasets or complex problems\n",
    "* Monitor validation performance to avoid overfitting, as too many estimators can degrade generalization\n",
    "\n",
    "\n",
    "`learning_rate`:\n",
    "\n",
    "* Common values to try: 0.1, 0.05, 0.01, 0.005\n",
    "* Use lower values (e.g., 0.01 or 0.005) if you are using many estimators (e.g., > 200)\n",
    "* Higher learning rates (e.g., 0.1) can be used with fewer estimators for faster convergence\n",
    "* Always balance the learning rate with `n_estimators` to prevent overfitting or underfitting\n",
    "\n",
    "\n",
    "`subsample`:\n",
    "\n",
    "* Common values: 0.7, 0.8, 0.9, 1.0\n",
    "* Use a value between `0.7` and `0.9` for improved generalization by introducing randomness\n",
    "* `1.0` uses the full dataset for each boosting round, potentially leading to overfitting\n",
    "* Reducing `subsample` can help reduce overfitting, especially in smaller datasets\n",
    "\n",
    "\n",
    "`max_features`:\n",
    "\n",
    "* Common values: `\"sqrt\"`, `\"log2\"`, or float (e.g., `0.3`, `0.5`)\n",
    "* `\"sqrt\"` (default) works well for classification tasks\n",
    "* Lower values (e.g., `0.3`) help reduce overfitting by limiting the number of features considered at each split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A17AkKJU8iuM",
   "metadata": {
    "id": "A17AkKJU8iuM"
   },
   "source": [
    "**Best practices for hyperparameter tuning in XGBoost:**\n",
    "\n",
    "`n_estimators`:\n",
    "\n",
    "* Start with 50 and increase in steps: 50,75,100,125.\n",
    "* Use more estimators (e.g., 150-250) when using lower learning rates\n",
    "* Monitor validation performance\n",
    "* High values improve learning but increase training time\n",
    "\n",
    "`subsample`:\n",
    "\n",
    "* Common values: 0.5, 0.7, 0.8, 1.0\n",
    "* Use `0.7–0.9` to introduce randomness and reduce overfitting\n",
    "* `1.0` uses the full dataset in each boosting round; may overfit on small datasets\n",
    "* Values < 0.5 are rarely useful unless dataset is very large\n",
    "\n",
    "`gamma`:\n",
    "\n",
    "* Try values: 0 (default), 1, 3, 5, 8\n",
    "* Controls minimum loss reduction needed for a split\n",
    "* Higher values make the algorithm more conservative (i.e., fewer splits)\n",
    "* Use values > 0 to regularize and reduce overfitting, especially on noisy data\n",
    "\n",
    "\n",
    "`colsample_bytree`:\n",
    "\n",
    "* Try values: 0.3, 0.5, 0.7, 1.0\n",
    "* Fraction of features sampled per tree\n",
    "* Lower values (e.g., 0.3 or 0.5) increase randomness and improve generalization\n",
    "* Use `1.0` when you want all features considered for every tree\n",
    "\n",
    "\n",
    "`colsample_bylevel`:\n",
    "\n",
    "* Try values: 0.3, 0.5, 0.7, 1.0\n",
    "* Fraction of features sampled at each tree level (i.e., per split depth)\n",
    "* Lower values help in regularization and reducing overfitting\n",
    "* Often used in combination with `colsample_bytree` for fine control over feature sampling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wyi8RcInCDDE",
   "metadata": {
    "id": "Wyi8RcInCDDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "D9JNnpxa4jau",
   "metadata": {
    "id": "D9JNnpxa4jau"
   },
   "source": [
    "## Model Performance Summary and Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jYiaUm-UCB_y",
   "metadata": {
    "id": "jYiaUm-UCB_y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "congressional-knock",
   "metadata": {
    "id": "congressional-knock"
   },
   "source": [
    "## Actionable Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y2HdXLmSJi8K",
   "metadata": {
    "id": "Y2HdXLmSJi8K"
   },
   "source": [
    "<font size=6 color='blue'>Power Ahead</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yZvo8CHcetWN",
    "empty-shanghai",
    "Lm7obbsV_RUT",
    "thorough-passion",
    "mq-1s9p-_aKl",
    "wooden-christian",
    "qBWlk20UBUAx",
    "D9JNnpxa4jau",
    "congressional-knock"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "WSL Py (ml-gpu, CUDA)",
   "language": "python",
   "name": "ml-gpu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
